{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6uHBJ4jesWDm4ROgGos0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaiazS/Transformer-based-Text-Generation-System/blob/main/Transformer_Model_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Data Loading and Text Data Pre-Processing**"
      ],
      "metadata": {
        "id": "zkX_heGkmMdm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY0F8c2yzKuv",
        "outputId": "8d809dc6-f033-4631-ca12-f1e1a809b373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2162, 3680, 4, 274, 224, 8, 651, 332, 652, 535, 35, 1268, 5, 164, 20, 21, 35, 1586, 973, 1587, 14, 69, 157, 21, 35, 2, 141, 128, 653, 789, 5, 32, 1588, 12, 169, 490, 110, 1416, 142, 21, 68, 55, 909, 25, 505, 1788, 151, 224, 10, 2, 2701]\n"
          ]
        }
      ],
      "source": [
        "#Loading Required Libraries and Text Dataset and Pre-processing the Text Dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "from tensorflow.keras.layers import Layer, Dense, Embedding, LayerNormalization, Dropout\n",
        "\n",
        "def load_data(file_path):\n",
        "\n",
        "  with open(file_path, 'r' , encoding = 'utf-8') as f:\n",
        "\n",
        "    text_data = f.read()\n",
        "\n",
        "  return text_data\n",
        "\n",
        "file_path = 'HP1.txt'\n",
        "\n",
        "text_data = load_data(file_path).lower()\n",
        "\n",
        "#1 - TOKENIZATION(TOKENIZING THE TEXT DATA)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = '<OOV>')\n",
        "\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "#Converting the Text into Sequences(Sentences)\n",
        "\n",
        "input_sequence = []\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "sequence_length = 50\n",
        "\n",
        "#First sequence_length tokens(input) -> Used for training the model.\n",
        "\n",
        "#Last token(target) -> Used as the label which the model tries to predict.\n",
        "\n",
        "#thus total of (50 + 1) in one input_sequence index\n",
        "\n",
        "for i in range(sequence_length,len(tokens)):\n",
        "\n",
        "  input_sequence.append(tokens[i - sequence_length : i + 1])\n",
        "\n",
        "print(input_sequence[0])\n",
        "\n",
        "#Padding sequences and splitting inputs and target tokens post which X will have the input tokens\n",
        "#and y will have the labels for those input tokens.\n",
        "\n",
        "input_sequence = np.array(pad_sequences(input_sequence, maxlen = sequence_length + 1, padding = 'pre'))\n",
        "\n",
        "X = input_sequence[:, :-1]\n",
        "\n",
        "y = input_sequence[:, -1]\n",
        "\n",
        "#2 - ENCODING THE TEXT DATA(VIA ONE-HOT ENCODING)\n",
        "\n",
        "#One hot encoding the labels, please not as there are other ways for encoding like\n",
        "\n",
        "#pre- trained word2vec encoding and so on.\n",
        "\n",
        "y = tf.keras.utils.to_categorical(y, num_classes = total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For every Self-Attention Head/Layer, the Query, Key and Value Matrix is going to have 64 Dimensions,\n",
        "and as we are having 8 Self-Attention layers/heads, the Combined Self-Attention layers/heads is going to have\n",
        "64 * 8 = 512 Dimensions in total."
      ],
      "metadata": {
        "id": "xl4JmGoRuhXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the Transformer Model**"
      ],
      "metadata": {
        "id": "oC43n2RLmErj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_Head_Attention(Layer):\n",
        "\n",
        "  def __init__(self, embed_dim, num_heads):\n",
        "\n",
        "    super(Multi_Head_Attention, self).__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim  # 512 Dimensions\n",
        "\n",
        "    self.num_heads = num_heads  # 8 Self-Attention Layers\n",
        "\n",
        "    self.projection_dim = embed_dim // num_heads #Query, Key and Value Matrices will be of 64 Dimensions in each Self-Attention Layer/Head.\n",
        "\n",
        "    self.query_dense = Dense(embed_dim)  #Dense layer for Query Matrix\n",
        "\n",
        "    self.key_dense = Dense(embed_dim)  #Dense layer for Key Matrix\n",
        "\n",
        "    self.value_dense = Dense(embed_dim) #Dense layer for Value Matrix\n",
        "\n",
        "    self.combine_all_layers = Dense(embed_dim) #Dense layer for the Combined Self-Attention Layers(512 Dimensions)\n",
        "\n",
        "\n",
        "  def compute_attention_score(self, query, key, value):\n",
        "\n",
        "    attention_score = tf.matmul(query, key, transpose_b = True) #Computing Dot Product of Query and Key(Transposed) Matrix\n",
        "\n",
        "    attention_score = attention_score / tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) #Scaling / Normalizing the Dot Product Result by dividing it by root of Query, Key or Value Matrix Dimensions of a Single Self-Attention Layer/Head and also converting the Result from Int to Float\n",
        "\n",
        "    attention_score_probability = tf.nn.softmax(attention_score, axis = -1) #Sum of values of each row representing a single token(word) will sum up to 1\n",
        "\n",
        "    final_attention_score = tf.matmul(attention_score, value)\n",
        "\n",
        "    return final_attention_score\n",
        "\n",
        "\n",
        "  def split_layers_to_each_individual_layer(self, input, batch_size):\n",
        "\n",
        "    # Updated Input Shape -> Batch_size, num_of words, num_of heads, projection_dim\n",
        "\n",
        "    input = tf.reshape(input, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "\n",
        "    #Shape we want thus updated -> batch_size, num_of heads, num_of_words(sequence_length), projection\n",
        "\n",
        "    #batch_size of (8 Self-attention layers of (4 words * 64 dimensions))\n",
        "\n",
        "    return tf.transpose(input, perm = [0, 2, 1, 3])\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    # Current Input Shape -> Batch_size, Num of words(sequence_length), Embedded Dims\n",
        "\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    query = self.query_dense(inputs)\n",
        "\n",
        "    key = self.key_dense(inputs)\n",
        "\n",
        "    value = self.value_dense(inputs)\n",
        "\n",
        "    query = self.split_layers_to_each_individual_layer(query, batch_size)\n",
        "\n",
        "    key = self.split_layers_to_each_individual_layer(key, batch_size)\n",
        "\n",
        "    value = self.split_layers_to_each_individual_layer(value, batch_size)\n",
        "\n",
        "    attention_score = self.compute_attention_score(query, key, value)\n",
        "\n",
        "    #Current Input Shape I have -> batch_size, num_of_heads, num_of_words(sequence_length), projection_dim\n",
        "\n",
        "    #Input Shape I want -> batch_size, num_of words(sequence_length), num_of heads, projection_dim\n",
        "\n",
        "    attention_score = tf.transpose(attention_score, perm = [0, 2, 1, 3])\n",
        "\n",
        "    final_combined_attention_score = tf.reshape(attention_score, (batch_size, -1, self.embed_dim))\n",
        "\n",
        "    return self.combine_all_layers(final_combined_attention_score)"
      ],
      "metadata": {
        "id": "MFnyJJz4m0Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Block(Layer):\n",
        "\n",
        "  def __init__(self, embed_dim, num_heads, simple_feed_forward_nn_dim, dropout_rate):\n",
        "\n",
        "      super(Transformer_Block, self).__init__()\n",
        "\n",
        "      self.attention = Multi_Head_Attention(embed_dim, num_heads)\n",
        "\n",
        "      self.simple_feed_forward_nn = tf.keras.Sequential([\n",
        "\n",
        "                                              Dense(simple_feed_forward_nn_dim, activation = 'relu'),\n",
        "\n",
        "                                              Dense(embed_dim)\n",
        "\n",
        "                                              ])\n",
        "\n",
        "   #Formula for Normalization - (input - mean) / standard deviation\n",
        "\n",
        "      self.normalization_layer_1 = LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "      self.normalization_layer_2 = LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "      self.dropout_layer_1 = Dropout(dropout_rate)\n",
        "\n",
        "      self.dropout_layer_2 = Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "  def call(self, inputs, training = False):\n",
        "\n",
        "     attention_output = self.attention(inputs)\n",
        "\n",
        "     attention_output = self.dropout_layer_1(attention_output, training = training)\n",
        "\n",
        "     output_1 = self.normalization_layer_1(inputs + attention_output)  #Residual Connection\n",
        "\n",
        "     feed_forward_nn_output = self.simple_feed_forward_nn(output_1)\n",
        "\n",
        "     feed_forward_nn_output = self.dropout_layer_2(feed_forward_nn_output, training = training)\n",
        "\n",
        "     return self.normalization_layer_2(output_1 + feed_forward_nn_output) #Residual Connection"
      ],
      "metadata": {
        "id": "-pb7BKRwLVN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenization_And_Positional_Embedding(Layer):\n",
        "\n",
        "  def __init__(self, max_len, vocab_size, embed_dim):\n",
        "\n",
        "    super(Tokenization_And_Positional_Embedding, self).__init__()\n",
        "\n",
        "    self.tokenization_layer = Embedding(input_dim = vocab_size, output_dim = embed_dim)\n",
        "\n",
        "    self.positional_embedding_layer = Embedding(input_dim = max_len, output_dim = embed_dim)\n",
        "\n",
        "\n",
        "  def call(self, word_input):\n",
        "\n",
        "    #The max sequence length the model can handle\n",
        "\n",
        "    max_len = tf.shape(word_input)[-1] #Sets max_len to the length of the input sequence\n",
        "\n",
        "    word_positions = tf.range(start = 0, limit = max_len, delta = 1) #Generating unique positions [0, 1, 2  up to max_len - 1]\n",
        "\n",
        "    word_positions = self.positional_embedding_layer(word_positions) #Each word position index is mapped to a trainable embedding of shape (max_len, embed_dim)\n",
        "\n",
        "    word_input = self.tokenization_layer(word_input) #Each token ID in word input is mapped to an embedding of shape batch_size, max_len, and embed_dim\n",
        "\n",
        "    return word_input + word_positions\n"
      ],
      "metadata": {
        "id": "Ln7Dkp52Zg60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelling the Whole Transformer\n",
        " Architecture, Compiling and Training the Model.**"
      ],
      "metadata": {
        "id": "xKpqxre0haGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Parameters\n",
        "\n",
        "embed_dim = 128 #Embedding Size\n",
        "\n",
        "num_heads = 4 #Number of attention heads\n",
        "\n",
        "simple_feed_forward_nn_dim = 512 #Feed Forward layer size\n",
        "\n",
        "max_len = sequence_length #Already previously defined as 50\n",
        "\n",
        "#Total words - 6662\n",
        "\n",
        "#Building the Model\n",
        "\n",
        "inputs = tf.keras.Input(shape =(max_len,))\n",
        "\n",
        "word_embedding_layer = Tokenization_And_Positional_Embedding(max_len, total_words, embed_dim)\n",
        "\n",
        "x = word_embedding_layer(inputs)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "transformer_block = Transformer_Block(embed_dim, num_heads, simple_feed_forward_nn_dim, dropout_rate = 0.2)\n",
        "\n",
        "x = transformer_block(x, training =True)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "x = x[:,-1,:]\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "x = Dense(total_words, activation = 'softmax')(x)\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "transformer_model = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "\n",
        "#Compiling the Model\n",
        "\n",
        "transformer_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "transformer_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "onTizH7Hhnra",
        "outputId": "a8419e18-7b3f-4459-aadd-7703d5cdede9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 50, 128)\n",
            "(None, 50, 128)\n",
            "(None, 128)\n",
            "(None, 6663)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ tokenization__and__positional__embe… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m859,264\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenization_And_Positional_Embedd…\u001b[0m │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer__block_6                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m198,272\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformer_Block\u001b[0m)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_6 (\u001b[38;5;33mGetItem\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6663\u001b[0m)                │         \u001b[38;5;34m859,527\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ tokenization__and__positional__embe… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">859,264</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Tokenization_And_Positional_Embedd…</span> │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer__block_6                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer_Block</span>)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6663</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">859,527</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model_performance = transformer_model.fit(X, y, batch_size = 32, epochs = 17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMesB-wClNXP",
        "outputId": "1cd86ea9-70d5-45e2-df73-30a8335f00fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 124ms/step - accuracy: 0.0433 - loss: 0.0222\n",
            "Epoch 2/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 127ms/step - accuracy: 0.1249 - loss: 0.0011\n",
            "Epoch 3/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 125ms/step - accuracy: 0.1662 - loss: 9.3766e-04\n",
            "Epoch 4/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 125ms/step - accuracy: 0.1898 - loss: 8.4480e-04\n",
            "Epoch 5/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 124ms/step - accuracy: 0.2112 - loss: 7.6875e-04\n",
            "Epoch 6/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 122ms/step - accuracy: 0.2362 - loss: 7.0413e-04\n",
            "Epoch 7/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 125ms/step - accuracy: 0.2756 - loss: 6.4419e-04\n",
            "Epoch 8/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 126ms/step - accuracy: 0.3426 - loss: 5.7955e-04\n",
            "Epoch 9/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 123ms/step - accuracy: 0.4124 - loss: 5.1423e-04\n",
            "Epoch 10/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 124ms/step - accuracy: 0.4920 - loss: 4.5356e-04\n",
            "Epoch 11/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 125ms/step - accuracy: 0.5738 - loss: 3.9384e-04\n",
            "Epoch 12/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 126ms/step - accuracy: 0.6306 - loss: 3.4668e-04\n",
            "Epoch 13/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 126ms/step - accuracy: 0.6858 - loss: 3.0563e-04\n",
            "Epoch 14/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 128ms/step - accuracy: 0.7253 - loss: 2.7219e-04\n",
            "Epoch 15/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 124ms/step - accuracy: 0.7621 - loss: 2.4243e-04\n",
            "Epoch 16/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 125ms/step - accuracy: 0.7863 - loss: 2.2205e-04\n",
            "Epoch 17/17\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 124ms/step - accuracy: 0.8102 - loss: 2.0462e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Function to Generate Text and Generating Text Post Training\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_length):\n",
        "\n",
        "  for _ in range(next_words):\n",
        "\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])([0])\n",
        "\n",
        "    token_list = pad_sequences([token_list], maxlen = max_sequence_length - 1, padding = 'pre')\n",
        "\n",
        "    predicted_text = transformer_model.predict(token_list, verbose = 0)\n",
        "\n",
        "    predicted_word = tokenizer.index_word[np.argmax(predicted_text)]\n",
        "\n",
        "    seed_text = seed_text + \" \" + predicted_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "seed_text = \"harry looked at\"\n",
        "\n",
        "model_generated_text = generate_text(seed_text, 25, sequence_length)\n",
        "\n",
        "print(len(model_generated_text))\n",
        "\n",
        "print(model_generated_text)"
      ],
      "metadata": {
        "id": "XIB_UAFS1BCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yzqsRx01AjSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference in this Simple Transformer Model In Comparision to ChatGPT :-\n",
        "\n",
        "*   Masked Attention:\n",
        "\n",
        "ChatGPT uses casual masking so that a word cannot see future words during training, and this current model uses\n",
        "regular attention, thus allowing the current model to see the entire sequence.\n",
        "\n",
        "*   Multiple Stacked Transformer Blocks:\n",
        "\n",
        "ChatGPT has many layers (e.g 12,24, 97 layers etc) and our current model has only one Transformer block.\n",
        "\n",
        "\n",
        "*   Tokenization and Byte-Pair Encoding(BPE):\n",
        "\n",
        "ChatGPT does not use simple tokenization, it uses Byte-Pair Encoding or WordPiece techniqiue for better Vocabulary handling and in the other hand, our current model uses only simple, basic Word Tokenization technique.\n",
        "\n",
        "*   Training on Large Datasets:\n",
        "\n",
        "ChatGPT is trained on hundreds of GBs of text data while on on the other hand, our current model is only trained on a book of Harry Potter.\n",
        "\n",
        "\n",
        "*   Decoding Strategies for Text Generation:\n",
        "\n",
        "ChatGPT uses sampling(Top-K, Nucleus Sampling) or Beam Search to generate text while on the other hand our current model does not have any Decoding Strategy."
      ],
      "metadata": {
        "id": "oCMODqrowz9P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjN4hVK72_ip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}